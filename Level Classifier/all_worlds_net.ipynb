{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.autograd as autograd         \n",
    "from torch import Tensor                  \n",
    "import torch.nn as nn                     \n",
    "import torch.nn.functional as F           \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level_image_list():    \n",
    "    images = []\n",
    "    for f in glob.iglob(\"images/?-?.png\"):\n",
    "        images.append(f)\n",
    "    images.sort()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_arrays(image_list):    \n",
    "    images_arrays = []\n",
    "    for f in image_list:\n",
    "        np_image = np.array(Image.open(f).convert('RGB')).astype(float)\n",
    "        images_arrays.append(np_image)\n",
    "        print(f, np_image.shape)\n",
    "    return images_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_level_slices():\n",
    "    image_set_array = np.array([])\n",
    "    target_set_array = np.array([])\n",
    "    stride = 18\n",
    "    for i, image in enumerate(get_image_arrays(get_level_image_list())):\n",
    "        windowed_image = image[:, (np.arange((image.shape[1] - 240) // stride) * stride)[:, np.newaxis] + np.arange(240)].transpose(1, 0, 2, 3)\n",
    "        image_set_array = np.vstack((image_set_array, windowed_image)) if image_set_array.size else windowed_image\n",
    "        target_set_array = np.concatenate((target_set_array, np.full((windowed_image.shape[0],), i))) if target_set_array.size else np.full((windowed_image.shape[0],), i)\n",
    "        print(windowed_image.shape)\n",
    "    return torch.from_numpy(image_set_array.transpose(0, 3, 1, 2)).type(torch.FloatTensor), torch.from_numpy(target_set_array).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/1-1.png (208, 3584, 3)\n",
      "images/1-2.png (208, 3072, 3)\n",
      "images/1-3.png (208, 2816, 3)\n",
      "images/1-4.png (208, 2560, 3)\n",
      "images/2-1.png (208, 3584, 3)\n",
      "images/2-2.png (208, 3072, 3)\n",
      "images/2-3.png (208, 3840, 3)\n",
      "images/2-4.png (208, 2560, 3)\n",
      "images/3-1.png (208, 3584, 3)\n",
      "images/3-2.png (208, 3584, 3)\n",
      "images/3-3.png (208, 2816, 3)\n",
      "images/3-4.png (208, 2560, 3)\n",
      "images/4-1.png (208, 3840, 3)\n",
      "images/4-2.png (208, 3584, 3)\n",
      "images/4-3.png (208, 2560, 3)\n",
      "images/4-4.png (208, 3072, 3)\n",
      "images/5-1.png (208, 3584, 3)\n",
      "images/5-2.png (208, 3584, 3)\n",
      "images/5-3.png (208, 2816, 3)\n",
      "images/5-4.png (208, 2560, 3)\n",
      "images/6-1.png (208, 3328, 3)\n",
      "images/6-2.png (208, 3840, 3)\n",
      "images/6-3.png (208, 3072, 3)\n",
      "images/6-4.png (208, 2560, 3)\n",
      "images/7-1.png (208, 3328, 3)\n",
      "images/7-2.png (208, 3072, 3)\n",
      "images/7-3.png (208, 3840, 3)\n",
      "images/7-4.png (208, 3584, 3)\n",
      "images/8-1.png (208, 6400, 3)\n",
      "images/8-2.png (208, 3840, 3)\n",
      "images/8-3.png (208, 3840, 3)\n",
      "images/8-4.png (208, 5120, 3)\n",
      "(185, 208, 240, 3)\n",
      "(157, 208, 240, 3)\n",
      "(143, 208, 240, 3)\n",
      "(128, 208, 240, 3)\n",
      "(185, 208, 240, 3)\n",
      "(157, 208, 240, 3)\n",
      "(200, 208, 240, 3)\n",
      "(128, 208, 240, 3)\n",
      "(185, 208, 240, 3)\n",
      "(185, 208, 240, 3)\n",
      "(143, 208, 240, 3)\n",
      "(128, 208, 240, 3)\n",
      "(200, 208, 240, 3)\n",
      "(185, 208, 240, 3)\n",
      "(128, 208, 240, 3)\n",
      "(157, 208, 240, 3)\n",
      "(185, 208, 240, 3)\n",
      "(185, 208, 240, 3)\n",
      "(143, 208, 240, 3)\n",
      "(128, 208, 240, 3)\n",
      "(171, 208, 240, 3)\n",
      "(200, 208, 240, 3)\n",
      "(157, 208, 240, 3)\n",
      "(128, 208, 240, 3)\n",
      "(171, 208, 240, 3)\n",
      "(157, 208, 240, 3)\n",
      "(200, 208, 240, 3)\n",
      "(185, 208, 240, 3)\n",
      "(342, 208, 240, 3)\n",
      "(200, 208, 240, 3)\n",
      "(200, 208, 240, 3)\n",
      "(271, 208, 240, 3)\n"
     ]
    }
   ],
   "source": [
    "image_tensor, target_tensor = create_level_slices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_dataset = TransformTensorDataset((image_tensor, target_tensor), transform=transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "val_split = .1\n",
    "test_split = .1\n",
    "shuffle_dataset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(level_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "test_split_i = int(np.floor(test_split * dataset_size))\n",
    "val_split_i = int(np.floor((test_split + val_split) * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    #np.random.seed(35)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices, test_indices = indices[val_split_i:], indices[test_split_i:val_split_i], indices[:test_split_i]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(level_dataset, batch_size=batch_size, \n",
    "                                           sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(level_dataset, batch_size=batch_size, \n",
    "                                           sampler=SubsetRandomSampler(val_indices))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(level_dataset, batch_size=batch_size,\n",
    "                                                sampler=SubsetRandomSampler(test_indices)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class LevelClassifier(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(LevelClassifier, self).__init__()\n",
    "\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            nn.Conv2d(3, 64, kernel_size=12, stride=4),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(256, 484, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(484),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(484, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(1536, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 32)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LevelClassifier().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 0...\n",
      "Epoch: 0 | Average train loss: 3.452666997909546 | (512 / 5617)\n",
      "Epoch: 0 | Average train loss: 3.320760130882263 | (1024 / 5617)\n",
      "Epoch: 0 | Average train loss: 3.10806941986084 | (1536 / 5617)\n",
      "Epoch: 0 | Average train loss: 2.8823697566986084 | (2048 / 5617)\n",
      "Epoch: 0 | Average train loss: 2.7127480506896973 | (2560 / 5617)\n",
      "Epoch: 0 | Average train loss: 2.5802059173583984 | (3072 / 5617)\n",
      "Epoch: 0 | Average train loss: 2.329314947128296 | (3584 / 5617)\n",
      "Epoch: 0 | Average train loss: 2.220212697982788 | (4096 / 5617)\n",
      "Epoch: 0 | Average train loss: 1.9938262104988098 | (4608 / 5617)\n",
      "Finished Epoch: 0 | Average train loss: 2.2423516386065043 | (5617/5617) | Validation Accuracy: 31.85053380782918\n",
      "\n",
      "Starting Epoch: 1...\n",
      "Epoch: 1 | Average train loss: 1.9432376623153687 | (512 / 5617)\n",
      "Epoch: 1 | Average train loss: 1.8099400401115417 | (1024 / 5617)\n",
      "Epoch: 1 | Average train loss: 1.6812840104103088 | (1536 / 5617)\n",
      "Epoch: 1 | Average train loss: 1.7343720197677612 | (2048 / 5617)\n",
      "Epoch: 1 | Average train loss: 1.548330008983612 | (2560 / 5617)\n",
      "Epoch: 1 | Average train loss: 1.5423539280891418 | (3072 / 5617)\n",
      "Epoch: 1 | Average train loss: 1.4239723086357117 | (3584 / 5617)\n",
      "Epoch: 1 | Average train loss: 1.2709049582481384 | (4096 / 5617)\n",
      "Epoch: 1 | Average train loss: 1.2514457702636719 | (4608 / 5617)\n",
      "Finished Epoch: 1 | Average train loss: 1.2948888093100466 | (5617/5617) | Validation Accuracy: 48.220640569395016\n",
      "\n",
      "Starting Epoch: 2...\n",
      "Epoch: 2 | Average train loss: 1.2429329752922058 | (512 / 5617)\n",
      "Epoch: 2 | Average train loss: 1.1231800317764282 | (1024 / 5617)\n",
      "Epoch: 2 | Average train loss: 1.1870035529136658 | (1536 / 5617)\n",
      "Epoch: 2 | Average train loss: 1.1341853737831116 | (2048 / 5617)\n",
      "Epoch: 2 | Average train loss: 0.9964281320571899 | (2560 / 5617)\n",
      "Epoch: 2 | Average train loss: 1.0482931733131409 | (3072 / 5617)\n",
      "Epoch: 2 | Average train loss: 0.9172883331775665 | (3584 / 5617)\n",
      "Epoch: 2 | Average train loss: 0.9590826630592346 | (4096 / 5617)\n",
      "Epoch: 2 | Average train loss: 0.9440890252590179 | (4608 / 5617)\n",
      "Finished Epoch: 2 | Average train loss: 0.8707266208729498 | (5617/5617) | Validation Accuracy: 52.84697508896797\n",
      "\n",
      "Starting Epoch: 3...\n",
      "Epoch: 3 | Average train loss: 0.8883169293403625 | (512 / 5617)\n",
      "Epoch: 3 | Average train loss: 0.8832258880138397 | (1024 / 5617)\n",
      "Epoch: 3 | Average train loss: 0.9059322774410248 | (1536 / 5617)\n",
      "Epoch: 3 | Average train loss: 0.8738124072551727 | (2048 / 5617)\n",
      "Epoch: 3 | Average train loss: 0.8697847425937653 | (2560 / 5617)\n",
      "Epoch: 3 | Average train loss: 0.8398629128932953 | (3072 / 5617)\n",
      "Epoch: 3 | Average train loss: 0.8511144518852234 | (3584 / 5617)\n",
      "Epoch: 3 | Average train loss: 0.7908146381378174 | (4096 / 5617)\n",
      "Epoch: 3 | Average train loss: 0.8895244002342224 | (4608 / 5617)\n",
      "Finished Epoch: 3 | Average train loss: 0.710290722391116 | (5617/5617) | Validation Accuracy: 55.693950177935946\n",
      "\n",
      "Starting Epoch: 4...\n",
      "Epoch: 4 | Average train loss: 0.7564089894294739 | (512 / 5617)\n",
      "Epoch: 4 | Average train loss: 0.7822385132312775 | (1024 / 5617)\n",
      "Epoch: 4 | Average train loss: 0.7621457576751709 | (1536 / 5617)\n",
      "Epoch: 4 | Average train loss: 0.7266615629196167 | (2048 / 5617)\n",
      "Epoch: 4 | Average train loss: 0.7234110534191132 | (2560 / 5617)\n",
      "Epoch: 4 | Average train loss: 0.7622082829475403 | (3072 / 5617)\n",
      "Epoch: 4 | Average train loss: 0.6795580089092255 | (3584 / 5617)\n",
      "Epoch: 4 | Average train loss: 0.6900231242179871 | (4096 / 5617)\n",
      "Epoch: 4 | Average train loss: 0.7197139859199524 | (4608 / 5617)\n",
      "Finished Epoch: 4 | Average train loss: 0.6018182429550847 | (5617/5617) | Validation Accuracy: 54.44839857651245\n",
      "\n",
      "Starting Epoch: 5...\n",
      "Epoch: 5 | Average train loss: 0.6421389281749725 | (512 / 5617)\n",
      "Epoch: 5 | Average train loss: 0.6398023068904877 | (1024 / 5617)\n",
      "Epoch: 5 | Average train loss: 0.6113748848438263 | (1536 / 5617)\n",
      "Epoch: 5 | Average train loss: 0.5758404731750488 | (2048 / 5617)\n",
      "Epoch: 5 | Average train loss: 0.6144699454307556 | (2560 / 5617)\n",
      "Epoch: 5 | Average train loss: 0.6104269921779633 | (3072 / 5617)\n",
      "Epoch: 5 | Average train loss: 0.5888085067272186 | (3584 / 5617)\n",
      "Epoch: 5 | Average train loss: 0.6388749778270721 | (4096 / 5617)\n",
      "Epoch: 5 | Average train loss: 0.5440769791603088 | (4608 / 5617)\n",
      "Finished Epoch: 5 | Average train loss: 0.49821911432022764 | (5617/5617) | Validation Accuracy: 62.09964412811388\n",
      "\n",
      "Starting Epoch: 6...\n",
      "Epoch: 6 | Average train loss: 0.5318991243839264 | (512 / 5617)\n",
      "Epoch: 6 | Average train loss: 0.4998064935207367 | (1024 / 5617)\n",
      "Epoch: 6 | Average train loss: 0.4607177972793579 | (1536 / 5617)\n",
      "Epoch: 6 | Average train loss: 0.543714851140976 | (2048 / 5617)\n",
      "Epoch: 6 | Average train loss: 0.46189090609550476 | (2560 / 5617)\n",
      "Epoch: 6 | Average train loss: 0.4827180951833725 | (3072 / 5617)\n",
      "Epoch: 6 | Average train loss: 0.5072350651025772 | (3584 / 5617)\n",
      "Epoch: 6 | Average train loss: 0.47787508368492126 | (4096 / 5617)\n",
      "Epoch: 6 | Average train loss: 0.46181055903434753 | (4608 / 5617)\n",
      "Finished Epoch: 6 | Average train loss: 0.4035901732985524 | (5617/5617) | Validation Accuracy: 51.06761565836299\n",
      "\n",
      "Starting Epoch: 7...\n",
      "Epoch: 7 | Average train loss: 0.449375182390213 | (512 / 5617)\n",
      "Epoch: 7 | Average train loss: 0.4733049273490906 | (1024 / 5617)\n",
      "Epoch: 7 | Average train loss: 0.42334355413913727 | (1536 / 5617)\n",
      "Epoch: 7 | Average train loss: 0.4150078445672989 | (2048 / 5617)\n",
      "Epoch: 7 | Average train loss: 0.4444095343351364 | (2560 / 5617)\n",
      "Epoch: 7 | Average train loss: 0.4331670105457306 | (3072 / 5617)\n",
      "Epoch: 7 | Average train loss: 0.4609023332595825 | (3584 / 5617)\n",
      "Epoch: 7 | Average train loss: 0.47691307961940765 | (4096 / 5617)\n",
      "Epoch: 7 | Average train loss: 0.4014086127281189 | (4608 / 5617)\n",
      "Finished Epoch: 7 | Average train loss: 0.36258679444793707 | (5617/5617) | Validation Accuracy: 63.52313167259786\n",
      "\n",
      "Starting Epoch: 8...\n",
      "Epoch: 8 | Average train loss: 0.43390268087387085 | (512 / 5617)\n",
      "Epoch: 8 | Average train loss: 0.34856224060058594 | (1024 / 5617)\n",
      "Epoch: 8 | Average train loss: 0.37264925241470337 | (1536 / 5617)\n",
      "Epoch: 8 | Average train loss: 0.38510943949222565 | (2048 / 5617)\n",
      "Epoch: 8 | Average train loss: 0.3574655205011368 | (2560 / 5617)\n",
      "Epoch: 8 | Average train loss: 0.4152442365884781 | (3072 / 5617)\n",
      "Epoch: 8 | Average train loss: 0.4206797778606415 | (3584 / 5617)\n",
      "Epoch: 8 | Average train loss: 0.37687601149082184 | (4096 / 5617)\n",
      "Epoch: 8 | Average train loss: 0.40546730160713196 | (4608 / 5617)\n",
      "Finished Epoch: 8 | Average train loss: 0.3204859726280849 | (5617/5617) | Validation Accuracy: 68.86120996441281\n",
      "\n",
      "Starting Epoch: 9...\n",
      "Epoch: 9 | Average train loss: 0.3315175920724869 | (512 / 5617)\n",
      "Epoch: 9 | Average train loss: 0.36827582120895386 | (1024 / 5617)\n",
      "Epoch: 9 | Average train loss: 0.29477737843990326 | (1536 / 5617)\n",
      "Epoch: 9 | Average train loss: 0.2934113144874573 | (2048 / 5617)\n",
      "Epoch: 9 | Average train loss: 0.3006403297185898 | (2560 / 5617)\n",
      "Epoch: 9 | Average train loss: 0.33837807178497314 | (3072 / 5617)\n",
      "Epoch: 9 | Average train loss: 0.30664514005184174 | (3584 / 5617)\n",
      "Epoch: 9 | Average train loss: 0.30238229036331177 | (4096 / 5617)\n",
      "Epoch: 9 | Average train loss: 0.3292543739080429 | (4608 / 5617)\n",
      "Finished Epoch: 9 | Average train loss: 0.26117581338120116 | (5617/5617) | Validation Accuracy: 75.62277580071175\n",
      "\n",
      "Starting Epoch: 10...\n",
      "Epoch: 10 | Average train loss: 0.2752586007118225 | (512 / 5617)\n",
      "Epoch: 10 | Average train loss: 0.3315461426973343 | (1024 / 5617)\n",
      "Epoch: 10 | Average train loss: 0.3030439019203186 | (1536 / 5617)\n",
      "Epoch: 10 | Average train loss: 0.2699021250009537 | (2048 / 5617)\n",
      "Epoch: 10 | Average train loss: 0.3373373746871948 | (2560 / 5617)\n",
      "Epoch: 10 | Average train loss: 0.3562776744365692 | (3072 / 5617)\n",
      "Epoch: 10 | Average train loss: 0.33657658100128174 | (3584 / 5617)\n",
      "Epoch: 10 | Average train loss: 0.32620739936828613 | (4096 / 5617)\n",
      "Epoch: 10 | Average train loss: 0.3565778285264969 | (4608 / 5617)\n",
      "Finished Epoch: 10 | Average train loss: 0.26367750502320314 | (5617/5617) | Validation Accuracy: 68.68327402135232\n",
      "\n",
      "Starting Epoch: 11...\n",
      "Epoch: 11 | Average train loss: 0.31296151876449585 | (512 / 5617)\n",
      "Epoch: 11 | Average train loss: 0.27910351753234863 | (1024 / 5617)\n",
      "Epoch: 11 | Average train loss: 0.3014880120754242 | (1536 / 5617)\n",
      "Epoch: 11 | Average train loss: 0.3124968111515045 | (2048 / 5617)\n",
      "Epoch: 11 | Average train loss: 0.2802498936653137 | (2560 / 5617)\n",
      "Epoch: 11 | Average train loss: 0.2866230756044388 | (3072 / 5617)\n",
      "Epoch: 11 | Average train loss: 0.2788293808698654 | (3584 / 5617)\n",
      "Epoch: 11 | Average train loss: 0.23384393006563187 | (4096 / 5617)\n",
      "Epoch: 11 | Average train loss: 0.2698848396539688 | (4608 / 5617)\n",
      "Finished Epoch: 11 | Average train loss: 0.23293684554817373 | (5617/5617) | Validation Accuracy: 80.96085409252669\n",
      "\n",
      "Starting Epoch: 12...\n",
      "Epoch: 12 | Average train loss: 0.24263526499271393 | (512 / 5617)\n",
      "Epoch: 12 | Average train loss: 0.24796418100595474 | (1024 / 5617)\n",
      "Epoch: 12 | Average train loss: 0.23077768087387085 | (1536 / 5617)\n",
      "Epoch: 12 | Average train loss: 0.2319600135087967 | (2048 / 5617)\n",
      "Epoch: 12 | Average train loss: 0.20873881876468658 | (2560 / 5617)\n",
      "Epoch: 12 | Average train loss: 0.20138422399759293 | (3072 / 5617)\n",
      "Epoch: 12 | Average train loss: 0.22101908922195435 | (3584 / 5617)\n",
      "Epoch: 12 | Average train loss: 0.2557554692029953 | (4096 / 5617)\n",
      "Epoch: 12 | Average train loss: 0.25400490313768387 | (4608 / 5617)\n",
      "Finished Epoch: 12 | Average train loss: 0.19089383978807187 | (5617/5617) | Validation Accuracy: 81.67259786476869\n",
      "\n",
      "Starting Epoch: 13...\n",
      "Epoch: 13 | Average train loss: 0.18219741433858871 | (512 / 5617)\n",
      "Epoch: 13 | Average train loss: 0.22080636024475098 | (1024 / 5617)\n",
      "Epoch: 13 | Average train loss: 0.2347237542271614 | (1536 / 5617)\n",
      "Epoch: 13 | Average train loss: 0.19577711820602417 | (2048 / 5617)\n",
      "Epoch: 13 | Average train loss: 0.20844857394695282 | (2560 / 5617)\n",
      "Epoch: 13 | Average train loss: 0.21566500514745712 | (3072 / 5617)\n",
      "Epoch: 13 | Average train loss: 0.23914658278226852 | (3584 / 5617)\n",
      "Epoch: 13 | Average train loss: 0.25582022219896317 | (4096 / 5617)\n",
      "Epoch: 13 | Average train loss: 0.18786272406578064 | (4608 / 5617)\n",
      "Finished Epoch: 13 | Average train loss: 0.17687542293766587 | (5617/5617) | Validation Accuracy: 77.22419928825623\n",
      "\n",
      "Starting Epoch: 14...\n",
      "Epoch: 14 | Average train loss: 0.1750948652625084 | (512 / 5617)\n",
      "Epoch: 14 | Average train loss: 0.2257046476006508 | (1024 / 5617)\n",
      "Epoch: 14 | Average train loss: 0.20165559649467468 | (1536 / 5617)\n",
      "Epoch: 14 | Average train loss: 0.23088882863521576 | (2048 / 5617)\n",
      "Epoch: 14 | Average train loss: 0.22904004156589508 | (2560 / 5617)\n",
      "Epoch: 14 | Average train loss: 0.2129214033484459 | (3072 / 5617)\n",
      "Epoch: 14 | Average train loss: 0.22218267619609833 | (3584 / 5617)\n",
      "Epoch: 14 | Average train loss: 0.21296808123588562 | (4096 / 5617)\n",
      "Epoch: 14 | Average train loss: 0.1775718331336975 | (4608 / 5617)\n",
      "Finished Epoch: 14 | Average train loss: 0.17209726231408454 | (5617/5617) | Validation Accuracy: 78.82562277580071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_FREQ = 2\n",
    "\n",
    "\n",
    "for epoch in range(15): \n",
    "    model.train()\n",
    "    \n",
    "    total_running_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    print(f\"Starting Epoch: {epoch}...\")\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        total_running_loss += loss.item()\n",
    "        if (i + 1) % PRINT_FREQ == 0:    # print every PRINT_FREQ batches\n",
    "            print(f\"Epoch: {epoch} | Average train loss: {running_loss / PRINT_FREQ} | ({batch_size * (i + 1)} / {len(train_loader.dataset)})\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "                  \n",
    "    print(f\"\"\"\\\n",
    "Finished Epoch: {epoch} | Average train loss: {total_running_loss / (len(train_loader.dataset) / batch_size)} | ({len(train_loader.dataset)}/{len(train_loader.dataset)})\\\n",
    " | Validation Accuracy: {100 * val_correct / val_total}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "test_correct = 0\n",
    "test_total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * test_correct / test_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-60170e5ad380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
